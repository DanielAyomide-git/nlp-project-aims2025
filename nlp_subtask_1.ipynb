{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjCDM2uyFpf9",
        "outputId": "3aa4170a-c0c8-416e-a123-8fda4caf014a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq58egFB1P2B",
        "outputId": "70913848-aff2-4774-d2e4-5dffd67d2342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIZJLpxO8NLS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainerCallback,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "BWhY37KFPVgx",
        "outputId": "08dd8a84-2df4-4f45-dc7b-080f73ce6345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/1s6t87yk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7be50eb339b0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# Disable wandb logging for this script\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAFyy-QMPW33"
      },
      "outputs": [],
      "source": [
        "my_pal = ['1f77b4', 'ff8c1a', '2ca02c', 'd62728', '9467bd', 'c5b300', 'e377c2', '17becf']\n",
        "my_pal = [f\"#{c}\" for c in my_pal]\n",
        "\n",
        "sns.reset_defaults() # useful when adjusting style a lot\n",
        "plt.rcParams['font.family']=['sans-serif']\n",
        "sns.set_theme(context=\"paper\", style=\"ticks\",\n",
        "              # palette=\"Set2\",\n",
        "              palette=my_pal,\n",
        "              rc={\n",
        "              \"pdf.fonttype\": 42,  # embed font in output\n",
        "              \"svg.fonttype\": \"none\",  # embed font in output\n",
        "              \"figure.facecolor\": \"white\",\n",
        "              \"figure.dpi\": 150,\n",
        "              \"axes.facecolor\": \"None\",\n",
        "              \"axes.spines.left\": True,\n",
        "              \"axes.spines.bottom\": True,\n",
        "              \"axes.spines.right\": False,\n",
        "              \"axes.spines.top\": False,\n",
        "          },\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "rZXTEg9T7Yn7",
        "outputId": "f34868e4-ace9-42b8-aea2-93d7f4176d81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        id  \\\n",
              "2356  swa_a0a8a7d3fadf4d25b8e732b78dd1f4f3   \n",
              "2891  swa_c2782f9e935e68e486062ffa71919077   \n",
              "2833  swa_ac33ea5f12a175f4e6bbf0333a610d06   \n",
              "67    swa_806334758786daada0bef3b6406e28c7   \n",
              "1235  swa_c11f130a0b352280bbfdaa336256879c   \n",
              "\n",
              "                                                   text  political  \\\n",
              "2356       baba tosha pongeziazimio uchaguzi mkuu Kenya          0   \n",
              "2891  matunda ya punda wa ghetto yalikuja kwenye bou...          0   \n",
              "2833  kwa hivyo nyani huyu anaweza kupata mcm lakini...          0   \n",
              "67    wee kuna rais wa marekani mhaya oscar award wi...          0   \n",
              "1235                              bubu bubu achana nami          0   \n",
              "\n",
              "      racial/ethnic  religious  gender/sexual  other stratify_key  \n",
              "2356              0          0              0      0    0_0_0_0_0  \n",
              "2891              0          0              0      0    0_0_0_0_0  \n",
              "2833              0          0              0      0    0_0_0_0_0  \n",
              "67                1          0              0      0    0_0_0_1_0  \n",
              "1235              0          0              0      1    0_0_0_0_1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00270de0-267a-49fb-bc46-85f544562c93\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>political</th>\n",
              "      <th>racial/ethnic</th>\n",
              "      <th>religious</th>\n",
              "      <th>gender/sexual</th>\n",
              "      <th>other</th>\n",
              "      <th>stratify_key</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2356</th>\n",
              "      <td>swa_a0a8a7d3fadf4d25b8e732b78dd1f4f3</td>\n",
              "      <td>baba tosha pongeziazimio uchaguzi mkuu Kenya</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0_0_0_0_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2891</th>\n",
              "      <td>swa_c2782f9e935e68e486062ffa71919077</td>\n",
              "      <td>matunda ya punda wa ghetto yalikuja kwenye bou...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0_0_0_0_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2833</th>\n",
              "      <td>swa_ac33ea5f12a175f4e6bbf0333a610d06</td>\n",
              "      <td>kwa hivyo nyani huyu anaweza kupata mcm lakini...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0_0_0_0_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>swa_806334758786daada0bef3b6406e28c7</td>\n",
              "      <td>wee kuna rais wa marekani mhaya oscar award wi...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0_0_0_1_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1235</th>\n",
              "      <td>swa_c11f130a0b352280bbfdaa336256879c</td>\n",
              "      <td>bubu bubu achana nami</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0_0_0_0_1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00270de0-267a-49fb-bc46-85f544562c93')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-00270de0-267a-49fb-bc46-85f544562c93 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-00270de0-267a-49fb-bc46-85f544562c93');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5123f1e7-60ee-496e-8eb5-b0ed24f1edad\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5123f1e7-60ee-496e-8eb5-b0ed24f1edad')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5123f1e7-60ee-496e-8eb5-b0ed24f1edad button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 5592,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5592,\n        \"samples\": [\n          \"swa_f0f077129b555e9af72ab158803a044b\",\n          \"swa_801dbf047a2e4e6e982804994e425227\",\n          \"swa_239f8e365b83b35a363aafe371bb8df4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5592,\n        \"samples\": [\n          \"ni mara ngapi majembe la kutombwa pussy matako hutazama picha yangu kwa siku\",\n          \"grinningfacerelax guy za meru na tharaka nithi tunatuma sasa hivi grinningfacewithsweat kenyadecides kenyaelect tharakanithi\",\n          \"local station leo asubuhi wakiulizwa mambo ya talli\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"political\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"racial/ethnic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"religious\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender/sexual\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"other\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stratify_key\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"0_0_1_1_0\",\n          \"1_0_0_1_1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train = pd.read_csv('subtask3/train/swa.csv')\n",
        "test = pd.read_csv('subtask3/dev/swa.csv')\n",
        "# fix this for this subtask\n",
        "train.head()\n",
        "train['stratify_key'] = train[['polarization']].apply(\n",
        "    lambda row: '_'.join(row.astype(str)), axis=1\n",
        ")\n",
        "\n",
        "train, val = train_test_split(\n",
        "    train,\n",
        "    test_size=0.2\n",
        ")\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4EOdrImO735"
      },
      "source": [
        "# new large\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmZkV8jRPFK5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "# ============================================\n",
        "# Configuration\n",
        "# ============================================\n",
        "CONFIG = {\n",
        "    'english': {\n",
        "        'model_name': 'cardiffnlp/twitter-roberta-base-hate-multiclass-latest',\n",
        "        'train_file': 'subtask1/train/swa.csv',\n",
        "        'test_file': 'subtask1/dev/swa.csv',\n",
        "        'output_name': 'eng'\n",
        "    },\n",
        "    'swahili': {\n",
        "        'model_name': 'metabloit/swahBERT',\n",
        "        'train_file': 'subtask1/train/swa.csv',\n",
        "        'test_file': 'subtask1/dev/swa.csv',\n",
        "        'output_name': 'swa'\n",
        "    }\n",
        "}\n",
        "# cardiffnlp/twitter-roberta-base-hate-multiclass-latest\n",
        "# metabloit/swahBERT\n",
        "# Tadesse/AfroXLMR-Social\n",
        "# Davlan/afro-xlmr-base\n",
        "# FacebookAI/xlm-roberta-base\n",
        "# castorini/afriteva_base\n",
        "# microsoft/deberta-v3-base\n",
        "\n",
        "# test for english\n",
        "# cardiffnlp/twitter-roberta-base-2022-154m\n",
        "LABEL_COLUMNS = [\"polarization\"]\n",
        "\n",
        "# Select language\n",
        "LANGUAGE = 'swahili'\n",
        "config = CONFIG[LANGUAGE]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLabelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "class MultiLabelTestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx]) if pd.notna(self.texts[idx]) else \"\"\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        return item\n",
        "\n",
        "# ============================================\n",
        "# Custom Trainer with Combined Metric\n",
        "# ============================================\n",
        "class CombinedMetricTrainer(Trainer):\n",
        "    \"\"\"Custom Trainer that optimizes for both F1 score and loss reduction\"\"\"\n",
        "    def __init__(self, *args, f1_weight=0.7, loss_weight=0.3, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.f1_weight = f1_weight\n",
        "        self.loss_weight = loss_weight\n",
        "        self.train_losses = []\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        \"\"\"Track training loss\"\"\"\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        self.train_losses.append(loss.item())\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
        "        \"\"\"Add combined score metric\"\"\"\n",
        "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
        "\n",
        "        f1 = metrics.get(f'{metric_key_prefix}_f1_macro', 0)\n",
        "        loss = metrics.get(f'{metric_key_prefix}_loss', 0)\n",
        "\n",
        "        normalized_loss = min(loss / 2.0, 1.0)\n",
        "        combined_score = (self.f1_weight * f1) - (self.loss_weight * normalized_loss)\n",
        "        metrics[f'{metric_key_prefix}_combined_score'] = combined_score\n",
        "\n",
        "        if self.train_losses:\n",
        "            avg_train_loss = np.mean(self.train_losses[-100:])\n",
        "            metrics[f'{metric_key_prefix}_train_loss_recent'] = avg_train_loss\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# ============================================\n",
        "# Enhanced Monitoring Callback\n",
        "# ============================================\n",
        "class EnhancedMonitoringCallback(TrainerCallback):\n",
        "    \"\"\"Callback to monitor and log F1, train loss, and eval loss\"\"\"\n",
        "    def __init__(self):\n",
        "        self.best_f1 = -float('inf')\n",
        "        self.best_eval_loss = float('inf')\n",
        "        self.best_train_loss = float('inf')\n",
        "        self.best_combined = -float('inf')\n",
        "        self.history = {\n",
        "            'epoch': [],\n",
        "            'train_loss': [],\n",
        "            'eval_loss': [],\n",
        "            'eval_f1': [],\n",
        "            'combined_score': []\n",
        "        }\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs and 'loss' in logs:\n",
        "            if logs['loss'] < self.best_train_loss:\n",
        "                self.best_train_loss = logs['loss']\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics is not None:\n",
        "            epoch = state.epoch\n",
        "            train_loss = metrics.get('train_loss', state.log_history[-1].get('loss', 0) if state.log_history else 0)\n",
        "            eval_loss = metrics.get('eval_loss', 0)\n",
        "            eval_f1 = metrics.get('eval_f1_macro', 0)\n",
        "            combined_score = metrics.get('eval_combined_score', 0)\n",
        "\n",
        "            self.history['epoch'].append(epoch)\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['eval_loss'].append(eval_loss)\n",
        "            self.history['eval_f1'].append(eval_f1)\n",
        "            self.history['combined_score'].append(combined_score)\n",
        "\n",
        "            if eval_f1 > self.best_f1:\n",
        "                self.best_f1 = eval_f1\n",
        "            if eval_loss < self.best_eval_loss:\n",
        "                self.best_eval_loss = eval_loss\n",
        "            if combined_score > self.best_combined:\n",
        "                self.best_combined = combined_score\n",
        "\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Epoch {epoch:.1f} Metrics:\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"  F1 Score:        {eval_f1:.4f} (Best: {self.best_f1:.4f})\")\n",
        "            print(f\"  Eval Loss:       {eval_loss:.4f} (Best: {self.best_eval_loss:.4f})\")\n",
        "            print(f\"  Train Loss:      {train_loss:.4f} (Best: {self.best_train_loss:.4f})\")\n",
        "            print(f\"  Combined Score:  {combined_score:.4f} (Best: {self.best_combined:.4f})\")\n",
        "\n",
        "            if eval_f1 == self.best_f1:\n",
        "                print(f\"  ✓ NEW BEST F1!\")\n",
        "            if eval_loss == self.best_eval_loss:\n",
        "                print(f\"  ✓ NEW BEST EVAL LOSS!\")\n",
        "            if combined_score == self.best_combined:\n",
        "                print(f\"  ✓ NEW BEST COMBINED SCORE!\")\n",
        "            print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ============================================\n",
        "# Early Stopping with Combined Metric\n",
        "# ============================================\n",
        "class CombinedEarlyStoppingCallback(TrainerCallback):\n",
        "    \"\"\"Early stopping based on combined F1 and loss metric\"\"\"\n",
        "    def __init__(self, patience=5):\n",
        "        self.patience = patience\n",
        "        self.best_combined_score = -float('inf')\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics is not None:\n",
        "            combined_score = metrics.get('eval_combined_score', -float('inf'))\n",
        "\n",
        "            if combined_score > self.best_combined_score:\n",
        "                self.best_combined_score = combined_score\n",
        "                self.patience_counter = 0\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "\n",
        "            if self.patience_counter >= self.patience:\n",
        "                print(f\"\\n⚠ Early stopping triggered after {self.patience} epochs without improvement\")\n",
        "                print(f\"Best combined score: {self.best_combined_score:.4f}\")\n",
        "                control.should_training_stop = True\n",
        "\n",
        "# ============================================\n",
        "# Compute Metrics for Multi-Label\n",
        "# ============================================\n",
        "def compute_metrics_multilabel(p):\n",
        "    \"\"\"Compute macro F1 for multi-label classification\"\"\"\n",
        "    probs = torch.sigmoid(torch.from_numpy(p.predictions))\n",
        "    preds = (probs > 0.5).int().numpy()\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro', zero_division=0)}\n",
        "\n",
        "# ============================================\n",
        "# Create Stratification Key for Multi-Label\n",
        "# ============================================\n",
        "def create_stratify_key(df, label_columns):\n",
        "    \"\"\"Create a stratification key for multi-label data\"\"\"\n",
        "    return df[label_columns].apply(lambda row: '_'.join(row.astype(str)), axis=1)\n",
        "\n",
        "# ============================================\n",
        "# K-Fold Cross-Validation with Enhanced Training\n",
        "# ============================================\n",
        "def train_kfold_cv(train_df, model_name, label_columns, n_splits=5, seed=42):\n",
        "    \"\"\"Train model using K-Fold Cross-Validation for multi-label classification\"\"\"\n",
        "\n",
        "    # Create stratification key\n",
        "    train_df['stratify_key'] = create_stratify_key(train_df, label_columns)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    fold_models = []\n",
        "    fold_scores = []\n",
        "    fold_metrics = []\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Starting {n_splits}-Fold Cross-Validation\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Total samples: {len(train_df)}\")\n",
        "    print(f\"Task: Multi-Label Classification ({len(label_columns)} labels)\")\n",
        "    print(f\"Optimization: Maximize F1 + Minimize Loss\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['stratify_key'])):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"FOLD {fold + 1}/{n_splits}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        train_fold = train_df.iloc[train_idx]\n",
        "        val_fold = train_df.iloc[val_idx]\n",
        "\n",
        "        print(f\"Train size: {len(train_fold)}, Val size: {len(val_fold)}\")\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = MultiLabelDataset(\n",
        "            train_fold['text'].tolist(),\n",
        "            train_fold[label_columns].values.tolist(),\n",
        "            tokenizer\n",
        "        )\n",
        "        val_dataset = MultiLabelDataset(\n",
        "            val_fold['text'].tolist(),\n",
        "            val_fold[label_columns].values.tolist(),\n",
        "            tokenizer\n",
        "        )\n",
        "\n",
        "        # Load config and set dropout parameters (BASIC - Fast Training)\n",
        "        model_config = AutoConfig.from_pretrained(model_name)\n",
        "        model_config.hidden_dropout_prob = 0.1  # Lower for faster training\n",
        "        model_config.attention_probs_dropout_prob = 0.1\n",
        "        model_config.classifier_dropout = 0.1\n",
        "        model_config.num_labels = len(label_columns)\n",
        "        model_config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "        # Initialize model with dropout\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            config=model_config,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        # BASIC training arguments - Fast and Simple\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f\"/content/outputs/fold_{fold+1}\",\n",
        "            num_train_epochs=4,  # Reduced to 4 epochs\n",
        "            learning_rate=3e-5,  # Standard learning rate\n",
        "            per_device_train_batch_size=32,\n",
        "            per_device_eval_batch_size=32,\n",
        "            weight_decay=0.01,  # Basic regularization\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_steps=50,\n",
        "            save_total_limit=1,  # Only keep best model\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"f1_macro\",  # Simple F1 metric\n",
        "            greater_is_better=True,\n",
        "            fp16=True,  # Speed up training\n",
        "            disable_tqdm=False,\n",
        "            report_to=\"none\"\n",
        "        )\n",
        "\n",
        "        # Initialize callbacks (BASIC - Simple monitoring)\n",
        "        monitoring_callback = EnhancedMonitoringCallback()\n",
        "\n",
        "        # Initialize STANDARD trainer (simpler, faster)\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            compute_metrics=compute_metrics_multilabel,\n",
        "            data_collator=DataCollatorWithPadding(tokenizer),\n",
        "            callbacks=[monitoring_callback]\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        print(f\"\\nTraining Fold {fold + 1}...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Final evaluation\n",
        "        eval_results = trainer.evaluate()\n",
        "        fold_score = eval_results['eval_f1_macro']\n",
        "        fold_eval_loss = eval_results['eval_loss']\n",
        "\n",
        "        fold_scores.append(fold_score)\n",
        "        fold_metrics.append({\n",
        "            'fold': fold + 1,\n",
        "            'f1': fold_score,\n",
        "            'eval_loss': fold_eval_loss,\n",
        "            'combined_score': fold_score,  # Use F1 as combined score for simplicity\n",
        "            'best_f1': monitoring_callback.best_f1,\n",
        "            'best_eval_loss': monitoring_callback.best_eval_loss,\n",
        "            'best_train_loss': monitoring_callback.best_train_loss\n",
        "        })\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Fold {fold + 1} Final Results:\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"  F1 Score:       {fold_score:.4f}\")\n",
        "        print(f\"  Eval Loss:      {fold_eval_loss:.4f}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Save model info\n",
        "        fold_models.append({\n",
        "            'model': model,\n",
        "            'tokenizer': tokenizer,\n",
        "            'score': fold_score,\n",
        "            'eval_loss': fold_eval_loss,\n",
        "            'combined_score': fold_score,  # Use F1 as combined score\n",
        "            'fold': fold + 1,\n",
        "            'history': monitoring_callback.history\n",
        "        })\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"CROSS-VALIDATION SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nPer-Fold Results:\")\n",
        "    print(f\"{'Fold':<8} {'F1 Score':<12} {'Eval Loss':<12} {'Combined':<12}\")\n",
        "    print(f\"{'-'*50}\")\n",
        "    for metrics in fold_metrics:\n",
        "        print(f\"{metrics['fold']:<8} {metrics['f1']:<12.4f} {metrics['eval_loss']:<12.4f} {metrics['combined_score']:<12.4f}\")\n",
        "\n",
        "    print(f\"\\nAggregate Statistics:\")\n",
        "    print(f\"  Mean F1:       {np.mean(fold_scores):.4f} ± {np.std(fold_scores):.4f}\")\n",
        "    print(f\"  Mean Eval Loss:{np.mean([m['eval_loss'] for m in fold_metrics]):.4f} ± {np.std([m['eval_loss'] for m in fold_metrics]):.4f}\")\n",
        "    print(f\"  Mean Combined: {np.mean([m['combined_score'] for m in fold_metrics]):.4f} ± {np.std([m['combined_score'] for m in fold_metrics]):.4f}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Plot results\n",
        "    plot_all_folds(fold_models, config['model_name'], LANGUAGE)\n",
        "\n",
        "    return fold_models, fold_metrics\n",
        "\n",
        "# ============================================\n",
        "# Plotting Function\n",
        "# ============================================\n",
        "def plot_all_folds(fold_models, model_name, language):\n",
        "    \"\"\"Plot training history for all folds with complementary colors\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    cfg = CONFIG[language]\n",
        "    # Extract short model name from full path\n",
        "    short_model_name = cfg['model_name'].split('/')[-1]\n",
        "\n",
        "    n_folds = len(fold_models)\n",
        "    colors = plt.cm.tab10(np.linspace(0, 0.9, n_folds))\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 6))\n",
        "    mosaic = [['losses', 'f1']]\n",
        "    axes = fig.subplot_mosaic(mosaic)\n",
        "\n",
        "    # Left plot: Losses for all folds\n",
        "    for idx, fold_model in enumerate(fold_models):\n",
        "        history = fold_model['history']\n",
        "        fold_num = fold_model['fold']\n",
        "        color = colors[idx]\n",
        "\n",
        "        axes['losses'].plot(history['epoch'], history['train_loss'],\n",
        "                           label=f'Fold {fold_num} Train',\n",
        "                           alpha=0.7,\n",
        "                           linestyle='--',\n",
        "                           color=color,\n",
        "                           linewidth=2)\n",
        "        axes['losses'].plot(history['epoch'], history['eval_loss'],\n",
        "                           label=f'Fold {fold_num} Val',\n",
        "                           alpha=0.9,\n",
        "                           color=color,\n",
        "                           linewidth=2)\n",
        "\n",
        "    axes['losses'].set_xlabel('Epoch', fontsize=11)\n",
        "    axes['losses'].set_ylabel('Loss', fontsize=11)\n",
        "    axes['losses'].set_title(f'Training & Validation Loss - {short_model_name}', fontsize=12, fontweight='bold')\n",
        "    axes['losses'].legend(fontsize=9, framealpha=0.9)\n",
        "    axes['losses'].grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Right plot: F1 scores for all folds\n",
        "    for idx, fold_model in enumerate(fold_models):\n",
        "        history = fold_model['history']\n",
        "        fold_num = fold_model['fold']\n",
        "        color = colors[idx]\n",
        "\n",
        "        axes['f1'].plot(history['epoch'], history['eval_f1'],\n",
        "                       label=f'Fold {fold_num}',\n",
        "                       marker='o',\n",
        "                       alpha=0.85,\n",
        "                       color=color,\n",
        "                       linewidth=2,\n",
        "                       markersize=6)\n",
        "\n",
        "    axes['f1'].set_xlabel('Epoch', fontsize=11)\n",
        "    axes['f1'].set_ylabel('F1 Score', fontsize=11)\n",
        "    axes['f1'].set_title(f'F1 Macro Score - {short_model_name}', fontsize=12, fontweight='bold')\n",
        "    axes['f1'].legend(fontsize=9, framealpha=0.9)\n",
        "    axes['f1'].grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # Save with model name in filename\n",
        "    plot_filename = f'multilabel_{cfg[\"output_name\"]}_{short_model_name}.pdf'\n",
        "    plt.savefig(plot_filename, dpi=200, format='pdf')\n",
        "    print(f\"\\n✓ Plot saved to: {plot_filename}\")\n",
        "    plt.show()\n",
        "\n",
        "# ============================================\n",
        "# Ensemble Prediction for Multi-Label\n",
        "# ============================================\n",
        "def ensemble_predict_multilabel(fold_models, test_dataset, method='weighted', threshold=0.5):\n",
        "    \"\"\"Make ensemble predictions for multi-label classification\"\"\"\n",
        "    all_predictions = []\n",
        "    weights = []\n",
        "\n",
        "    for i, fold_model in enumerate(fold_models):\n",
        "        print(f\"Getting predictions from Fold {i+1} (F1: {fold_model['score']:.4f})...\")\n",
        "\n",
        "        model = fold_model['model']\n",
        "        tokenizer = fold_model['tokenizer']\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            data_collator=DataCollatorWithPadding(tokenizer)\n",
        "        )\n",
        "\n",
        "        predictions = trainer.predict(test_dataset)\n",
        "        probs = torch.sigmoid(torch.tensor(predictions.predictions))\n",
        "        all_predictions.append(probs.numpy())\n",
        "\n",
        "        if method == 'weighted':\n",
        "            weights.append(fold_model['score'])\n",
        "        elif method == 'weighted_combined':\n",
        "            weights.append(fold_model['combined_score'])\n",
        "        else:\n",
        "            weights.append(1.0)\n",
        "\n",
        "    # Normalize weights\n",
        "    weights = np.array(weights)\n",
        "    weights = weights / weights.sum()\n",
        "\n",
        "    print(f\"\\nEnsemble weights: {weights}\")\n",
        "\n",
        "    # Weighted average of probabilities\n",
        "    weighted_probs = np.zeros_like(all_predictions[0])\n",
        "    for pred, weight in zip(all_predictions, weights):\n",
        "        weighted_probs += pred * weight\n",
        "\n",
        "    # Apply threshold\n",
        "    final_predictions = (weighted_probs > threshold).astype(int)\n",
        "\n",
        "    return final_predictions\n",
        "\n",
        "# ============================================\n",
        "# Save Models in Compressed Format\n",
        "# ============================================\n",
        "def save_fold_models(fold_models, fold_metrics, language):\n",
        "    \"\"\"Save all fold models and metrics in a single compressed file\"\"\"\n",
        "    cfg = CONFIG[language]\n",
        "\n",
        "    # Extract short model name from full path\n",
        "    short_model_name = cfg['model_name'].split('/')[-1]\n",
        "\n",
        "    # Create temporary directory for models\n",
        "    temp_dir = f\"/content/temp_models_{cfg['output_name']}\"\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Save best model\n",
        "    best_fold_idx = np.argmax([m['combined_score'] for m in fold_metrics])\n",
        "    best_fold_model = fold_models[best_fold_idx]\n",
        "\n",
        "    best_model_dir = os.path.join(temp_dir, \"best_model\")\n",
        "    os.makedirs(best_model_dir, exist_ok=True)\n",
        "    best_fold_model['model'].save_pretrained(best_model_dir)\n",
        "    best_fold_model['tokenizer'].save_pretrained(best_model_dir)\n",
        "\n",
        "    print(f\"\\n✓ Best model prepared (Fold {best_fold_model['fold']})\")\n",
        "    print(f\"✓ F1 Score: {best_fold_model['score']:.4f}\")\n",
        "    print(f\"✓ Combined Score: {best_fold_model['combined_score']:.4f}\")\n",
        "\n",
        "    # Save all fold models\n",
        "    for fold_model in fold_models:\n",
        "        fold_dir = os.path.join(temp_dir, f\"fold_{fold_model['fold']}\")\n",
        "        os.makedirs(fold_dir, exist_ok=True)\n",
        "        fold_model['model'].save_pretrained(fold_dir)\n",
        "        fold_model['tokenizer'].save_pretrained(fold_dir)\n",
        "        print(f\"✓ Prepared Fold {fold_model['fold']}\")\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_path = os.path.join(temp_dir, \"fold_metrics.json\")\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        json.dump(fold_metrics, f, indent=2)\n",
        "\n",
        "    # Save model info\n",
        "    model_info = {\n",
        "        'language': language,\n",
        "        'model_name': cfg['model_name'],\n",
        "        'short_model_name': short_model_name,\n",
        "        'best_fold': best_fold_model['fold'],\n",
        "        'best_f1': best_fold_model['score'],\n",
        "        'best_combined_score': best_fold_model['combined_score'],\n",
        "        'n_folds': len(fold_models),\n",
        "        'label_columns': LABEL_COLUMNS\n",
        "    }\n",
        "    info_path = os.path.join(temp_dir, \"model_info.json\")\n",
        "    with open(info_path, 'w') as f:\n",
        "        json.dump(model_info, f, indent=2)\n",
        "\n",
        "    # Compress everything with model name in filename\n",
        "    zip_filename = f\"multilabel_{cfg['output_name']}_{short_model_name}.zip\"\n",
        "    final_path = f\"/content/drive/MyDrive/NLP/{zip_filename}\"\n",
        "\n",
        "    print(f\"\\nCompressing all models...\")\n",
        "    with zipfile.ZipFile(final_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(temp_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, temp_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "    # Clean up temp directory\n",
        "    import shutil\n",
        "    shutil.rmtree(temp_dir)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"✓ All models saved to compressed file:\")\n",
        "    print(f\"  {final_path}\")\n",
        "    print(f\"  Model: {short_model_name}\")\n",
        "    print(f\"  Language: {language}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ============================================\n",
        "# LOAD SAVED MODEL FOR PREDICTION\n",
        "# ============================================\n",
        "def load_saved_model(language='english', model_name=None):\n",
        "    \"\"\"\n",
        "    Load the best saved model from compressed file\n",
        "\n",
        "    Args:\n",
        "        language: 'english' or 'swahili'\n",
        "        model_name: Optional specific model name (e.g., 'xlm-roberta-base').\n",
        "                   If None, uses the model_name from CONFIG\n",
        "\n",
        "    Returns:\n",
        "        model, tokenizer, model_info\n",
        "    \"\"\"\n",
        "    cfg = CONFIG[language]\n",
        "\n",
        "    # Use provided model_name or extract from config\n",
        "    if model_name is None:\n",
        "        short_model_name = cfg['model_name'].split('/')[-1]\n",
        "    else:\n",
        "        short_model_name = model_name.split('/')[-1] if '/' in model_name else model_name\n",
        "\n",
        "    zip_path = f\"/content/drive/MyDrive/NLP/multilabel_{cfg['output_name']}_{short_model_name}.zip\"\n",
        "\n",
        "    print(f\"Loading model from: {zip_path}\")\n",
        "\n",
        "    # Extract to temp directory\n",
        "    temp_extract_dir = f\"/content/temp_extract_{cfg['output_name']}\"\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
        "        zipf.extractall(temp_extract_dir)\n",
        "\n",
        "    # Load model info\n",
        "    with open(os.path.join(temp_extract_dir, \"model_info.json\"), 'r') as f:\n",
        "        model_info = json.load(f)\n",
        "\n",
        "    # Load best model\n",
        "    best_model_dir = os.path.join(temp_extract_dir, \"best_model\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(best_model_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(best_model_dir)\n",
        "\n",
        "    print(f\"✓ Model loaded successfully!\")\n",
        "    print(f\"  Model: {model_info['short_model_name']}\")\n",
        "    print(f\"  Best Fold: {model_info['best_fold']}\")\n",
        "    print(f\"  F1 Score: {model_info['best_f1']:.4f}\")\n",
        "\n",
        "    return model, tokenizer, model_info\n",
        "\n",
        "# ============================================\n",
        "# LOAD ALL FOLD MODELS FOR ENSEMBLE\n",
        "# ============================================\n",
        "def load_all_fold_models(language='english', model_name=None):\n",
        "    \"\"\"\n",
        "    Load all saved fold models from compressed file for ensemble prediction\n",
        "\n",
        "    Args:\n",
        "        language: 'english' or 'swahili'\n",
        "        model_name: Optional specific model name (e.g., 'xlm-roberta-base').\n",
        "                   If None, uses the model_name from CONFIG\n",
        "\n",
        "    Returns:\n",
        "        fold_models: List of model dictionaries\n",
        "    \"\"\"\n",
        "    cfg = CONFIG[language]\n",
        "\n",
        "    # Use provided model_name or extract from config\n",
        "    if model_name is None:\n",
        "        short_model_name = cfg['model_name'].split('/')[-1]\n",
        "    else:\n",
        "        short_model_name = model_name.split('/')[-1] if '/' in model_name else model_name\n",
        "\n",
        "    zip_path = f\"/content/drive/MyDrive/NLP/multilabel_{cfg['output_name']}_{short_model_name}.zip\"\n",
        "\n",
        "    print(f\"\\nLoading all fold models from: {zip_path}\")\n",
        "\n",
        "    # Extract to temp directory\n",
        "    temp_extract_dir = f\"/content/temp_extract_{cfg['output_name']}\"\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
        "        zipf.extractall(temp_extract_dir)\n",
        "\n",
        "    # Load model info and metrics\n",
        "    with open(os.path.join(temp_extract_dir, \"model_info.json\"), 'r') as f:\n",
        "        model_info = json.load(f)\n",
        "\n",
        "    with open(os.path.join(temp_extract_dir, \"fold_metrics.json\"), 'r') as f:\n",
        "        fold_metrics = json.load(f)\n",
        "\n",
        "    n_folds = model_info['n_folds']\n",
        "    fold_models = []\n",
        "\n",
        "    print(f\"Loading {n_folds} fold models for {language} ({short_model_name})...\")\n",
        "\n",
        "    for fold_num in range(1, n_folds + 1):\n",
        "        fold_dir = os.path.join(temp_extract_dir, f\"fold_{fold_num}\")\n",
        "\n",
        "        print(f\"  Loading Fold {fold_num}...\")\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(fold_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(fold_dir)\n",
        "\n",
        "        fold_metric = fold_metrics[fold_num - 1]\n",
        "\n",
        "        fold_models.append({\n",
        "            'model': model,\n",
        "            'tokenizer': tokenizer,\n",
        "            'fold': fold_num,\n",
        "            'score': fold_metric['f1'],\n",
        "            'eval_loss': fold_metric['eval_loss'],\n",
        "            'combined_score': fold_metric['combined_score']\n",
        "        })\n",
        "\n",
        "        print(f\"  ✓ Fold {fold_num} loaded (F1: {fold_metric['f1']:.4f}, Loss: {fold_metric['eval_loss']:.4f})\")\n",
        "\n",
        "    print(f\"\\n✓ All {n_folds} models loaded successfully!\")\n",
        "\n",
        "    return fold_models\n",
        "\n",
        "# ============================================\n",
        "# PREDICT WITH SAVED MODEL\n",
        "# ============================================\n",
        "def predict_with_saved_model(test_csv_path, language='english', model_name=None,\n",
        "                             output_csv='predictions.csv', threshold=0.5):\n",
        "    \"\"\"\n",
        "    Make predictions using saved best model\n",
        "\n",
        "    Args:\n",
        "        test_csv_path: Path to test CSV file\n",
        "        language: 'english' or 'swahili'\n",
        "        model_name: Optional specific model name. If None, uses CONFIG model_name\n",
        "        output_csv: Output CSV filename\n",
        "        threshold: Prediction threshold for multi-label\n",
        "    \"\"\"\n",
        "    # Load saved model\n",
        "    model, tokenizer, model_info = load_saved_model(language, model_name)\n",
        "\n",
        "    # Load test data\n",
        "    test = pd.read_csv(test_csv_path)\n",
        "    print(f\"Test size: {len(test)}\")\n",
        "\n",
        "    # Create test dataset\n",
        "    test_dataset = MultiLabelTestDataset(test['text'].tolist(), tokenizer)\n",
        "\n",
        "    # Create trainer for prediction\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer)\n",
        "    )\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"Making predictions...\")\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    probs = torch.sigmoid(torch.tensor(predictions.predictions))\n",
        "    predicted_labels = (probs > threshold).int().numpy()\n",
        "\n",
        "    # Save results\n",
        "    label_cols = model_info['label_columns']\n",
        "    results_df = pd.DataFrame({'id': test['id']})\n",
        "    for i, col in enumerate(label_cols):\n",
        "        results_df[col] = predicted_labels[:, i]\n",
        "\n",
        "    results_df.to_csv(output_csv, index=False)\n",
        "\n",
        "    print(f\"✓ Predictions saved to: {output_csv}\")\n",
        "    print(f\"\\nSample predictions:\")\n",
        "    print(results_df.head())\n",
        "    print(f\"\\nPrediction distribution:\")\n",
        "    print(results_df[label_cols].sum())\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# ============================================\n",
        "# ENSEMBLE PREDICT WITH SAVED MODELS\n",
        "# ============================================\n",
        "def ensemble_predict_with_saved_models(test_csv_path, language='english', model_name=None,\n",
        "                                       method='weighted_combined',\n",
        "                                       output_csv='ensemble_predictions.csv',\n",
        "                                       threshold=0.5):\n",
        "    \"\"\"\n",
        "    Make ensemble predictions using all saved fold models\n",
        "\n",
        "    Args:\n",
        "        test_csv_path: Path to test CSV file\n",
        "        language: 'english' or 'swahili'\n",
        "        model_name: Optional specific model name. If None, uses CONFIG model_name\n",
        "        method: 'average', 'weighted', or 'weighted_combined'\n",
        "        output_csv: Output CSV filename\n",
        "        threshold: Prediction threshold for multi-label\n",
        "    \"\"\"\n",
        "    # Load all fold models\n",
        "    fold_models = load_all_fold_models(language, model_name)\n",
        "\n",
        "    # Load test data\n",
        "    test = pd.read_csv(test_csv_path)\n",
        "    print(f\"\\nTest size: {len(test)}\")\n",
        "\n",
        "    # Create test dataset\n",
        "    tokenizer = fold_models[0]['tokenizer']\n",
        "    test_dataset = MultiLabelTestDataset(test['text'].tolist(), tokenizer)\n",
        "\n",
        "    # Make ensemble predictions\n",
        "    print(f\"\\nMaking ensemble predictions with method: {method}\")\n",
        "    final_predictions = ensemble_predict_multilabel(fold_models, test_dataset, method=method, threshold=threshold)\n",
        "\n",
        "    # Save results\n",
        "      results_df = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'polarization': final_predictions[:, 0],\n",
        "    })\n",
        "\n",
        "\n",
        "    results_df.to_csv(output_csv, index=False)\n",
        "\n",
        "    print(f\"\\n✓ Ensemble predictions saved to: {output_csv}\")\n",
        "    print(f\"\\nSample predictions:\")\n",
        "    print(results_df.head(10))\n",
        "    print(f\"\\nPrediction distribution:\")\n",
        "    print(results_df[LABEL_COLUMNS].sum())\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# ============================================\n",
        "# CONTINUE TRAINING SAVED MODEL\n",
        "# ============================================\n",
        "def continue_training(train_csv_path, language='english', model_name=None, additional_epochs=5):\n",
        "    \"\"\"\n",
        "    Continue training a saved model\n",
        "\n",
        "    Args:\n",
        "        train_csv_path: Path to training CSV\n",
        "        language: 'english' or 'swahili'\n",
        "        model_name: Optional specific model name. If None, uses CONFIG model_name\n",
        "        additional_epochs: Number of additional epochs to train\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Load saved model\n",
        "    model, tokenizer, model_info = load_saved_model(language, model_name)\n",
        "\n",
        "    # Load training data\n",
        "    train = pd.read_csv(train_csv_path)\n",
        "    print(f\"Train size: {len(train)}\")\n",
        "\n",
        "    # Create stratification key\n",
        "    train['stratify_key'] = create_stratify_key(train, LABEL_COLUMNS)\n",
        "\n",
        "    # Split into train/val\n",
        "    train_split, val_split = train_test_split(\n",
        "        train,\n",
        "        test_size=0.2,\n",
        "        stratify=train['stratify_key'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = MultiLabelDataset(\n",
        "        train_split['text'].tolist(),\n",
        "        train_split[LABEL_COLUMNS].values.tolist(),\n",
        "        tokenizer\n",
        "    )\n",
        "    val_dataset = MultiLabelDataset(\n",
        "        val_split['text'].tolist(),\n",
        "        val_split[LABEL_COLUMNS].values.tolist(),\n",
        "        tokenizer\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"/content/outputs/continued_training\",\n",
        "        num_train_epochs=additional_epochs,\n",
        "        learning_rate=3e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=32,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=25,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True,\n",
        "        fp16=True,\n",
        "        dataloader_num_workers=2,\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics_multilabel,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer)\n",
        "    )\n",
        "\n",
        "    # Continue training\n",
        "    print(f\"\\nContinuing training for {additional_epochs} epochs...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"\\n✓ Final F1 Score: {eval_results['eval_f1_macro']:.4f}\")\n",
        "\n",
        "    # Save updated model\n",
        "    cfg = CONFIG[language]\n",
        "    short_model_name = model_info['short_model_name']\n",
        "    updated_model_dir = f\"/content/temp_updated_model\"\n",
        "    os.makedirs(updated_model_dir, exist_ok=True)\n",
        "    trainer.save_model(updated_model_dir)\n",
        "    tokenizer.save_pretrained(updated_model_dir)\n",
        "\n",
        "    # Compress updated model\n",
        "    zip_filename = f\"multilabel_{cfg['output_name']}_{short_model_name}_updated.zip\"\n",
        "    final_path = f\"/content/drive/MyDrive/NLP{zip_filename}\"\n",
        "\n",
        "    with zipfile.ZipFile(final_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(updated_model_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, updated_model_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "    print(f\"✓ Updated model saved to: {final_path}\")\n",
        "\n",
        "    return trainer\n",
        "\n",
        "# ============================================\n",
        "# Main Training Pipeline\n",
        "# ============================================\n",
        "def main():\n",
        "    print(f\"Loading data for {LANGUAGE}...\")\n",
        "    train = pd.read_csv(config['train_file'])\n",
        "    test = pd.read_csv(config['test_file'])\n",
        "\n",
        "    print(f\"Train size: {len(train)}\")\n",
        "    print(f\"Test size: {len(test)}\")\n",
        "    print(f\"\\nLabel distribution:\")\n",
        "    print(train[LABEL_COLUMNS].sum())\n",
        "\n",
        "    # Train with K-Fold CV\n",
        "    fold_models, fold_metrics = train_kfold_cv(\n",
        "        train,\n",
        "        config['model_name'],\n",
        "        LABEL_COLUMNS,\n",
        "        n_splits=5,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Save models\n",
        "    save_fold_models(fold_models, fold_metrics, LANGUAGE)\n",
        "\n",
        "    # Create test dataset\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "    test_dataset = MultiLabelTestDataset(test['text'].tolist(), tokenizer)\n",
        "\n",
        "    # Ensemble predictions\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Making Weighted Ensemble Predictions\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    final_predictions = ensemble_predict_multilabel(\n",
        "        fold_models,\n",
        "        test_dataset,\n",
        "        method='weighted_combined',\n",
        "        threshold=0.5\n",
        "    )\n",
        "\n",
        "    # Save predictions\n",
        "     results_df = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'polarization': final_predictions[:, 0],\n",
        "    })\n",
        "\n",
        "\n",
        "    os.makedirs('subtask_1', exist_ok=True)\n",
        "    csv_path = f'subtask_1/pred_{config[\"output_name\"]}.csv'\n",
        "    results_df.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"\\n✓ Saved predictions to {csv_path}\")\n",
        "    print(f\"\\nSample predictions:\")\n",
        "    print(results_df.head(10))\n",
        "    print(f\"\\nPrediction distribution:\")\n",
        "    print(results_df[LABEL_COLUMNS].sum())\n",
        "\n",
        "    # Compress\n",
        "    zip_filename = f'subtask_1_{config[\"output_name\"]}_ensemble.zip'\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk('subtask_2'):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.join(os.path.basename(root), file)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "    print(f\"✓ Created compressed file: {zip_filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "re62yIZdl8Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABiN7nCvcBce"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaP3XAiPLqsw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}